# 模型背景

原论文的模型所立足的基础模型（One-Shot NAS）的用于从一个大型**超网络**（Hyper-network）中选出最优的**分网络**（Sub-network）。该基础模型的核心在于一个优中选优的两步优化过程：
1. 利用*训练集*（Training Dataset）最优化**超网络**的权重：每一个**批次**（Batch）的数据仅用于优化一个随机的**分网络**下的权重
2. 从所有**分网络**中利用*检验集*（Validation Dataset）选出最优的作为结果

以上基础模型的优势是仅进行一轮训练所带来的计算量的降低；但同时，单次训练也造成了这一模型的最大问题：最初训练的**超网络**最优权重为**分网络**所共享，但对于**分网络**来说未必是最优的。因此，原论文所聚焦的核心问题在于在仍然在仅进行一轮训练的前提下实现**超网络**权重的不断优化。

# 模型思想

原文网络通过四层最优机制实现**超网络**权重的不断优化：

`注：以下“网络（Network）”与“路径（Path）”是完全等同的概念`

1. 构造包含**先进路径**（Prioritized Path）的**先进路径库**（Prioritized Path Board），通过优胜劣汰（不断选出最优**分路径**替代最劣**先进路径**）保证**先进路径库**中的路径始终出于最优化状态
2. 通过**匹配网络**（Meta-network）为待优化**分路径**匹配**互补性**（complementarity）最强的**先进路径**
3. 借助**先进路径**，利用*知识蒸馏*（Knowledge Distillation）与普通梯度优化配合训练**分路径**下的权重
4. 利用**先进路径库**中**先进路径**在*检验集*下的精确度优化**匹配网络**的权重


# 算法结构

```
for t in 0~T if 不收敛:
    选择任意分路径#0
    匹配最互补先进路径#2
    知识蒸馏训练分路径下权重#3
    优化先进路径库#1
    if t%tau = 0:
        优化匹配网络#4
```

# 符号说明

<center>

|变量|符号|公式|
|:--:|:--:|:--:|
|匹配网络训练周期|$\tau$|-|
|总训练步长|$T$|-|
|训练步|$t$|-|
|训练集|$\{{\pmb{X}_t}_i, {\pmb{y}_t}_i\}$|-|
|检测集|$\{{\pmb{X}_v}_i, {\pmb{y}_v}_i\}$|-|
|超网络|$\mathcal{N}$|-|
|超网络权重|$W$|-|
|分路径|$\alpha_k$|-|
|分路径权重|$w_{\alpha_k}$|-|
|超网络函数|-|$y = \mathcal{N}(\pmb{x}, \alpha, w_\alpha)$|
|先进路径库|$\mathbb{B}$||
|先进路径|${\hat\alpha}_k$|$\mathbb{B} = \{{\hat\alpha}_k\}$|
|匹配网络|$\mathcal{M}$|-|
|匹配网络权重|$\theta$|-|
|网络互补性|$\rho$|$\rho = \mathcal{N}(\pmb{y}_1, \pmb{y}_2, \theta)$|
</center>

# 重要公式

#1 #2 #3 #4 分别各对应一公式，见原文